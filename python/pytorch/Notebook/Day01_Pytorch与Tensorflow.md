## 1、Pytorch介绍

​	Pytorch是torch的python版本，是由Facebook开源的神经网络框架，专门==针对 GPU 加速的深度神经网络（DNN）==编程。Torch 是一个经典的对多维矩阵数据进行操作的张量（tensor ）库，在机器学习和其他数学密集型应用有广泛应用。与Tensorflow的静态计算图不同，pytorch的计算图是动态的，可以根据计算需要实时改变计算图。

## 2、静态图机制和动态图机制

​	深度学习就是对==张量==进行一系列的操作，随着操作种类和数量的增多，会出现各种值得思考的问题。比如多个操作之间是否可以并行，如何协同底层的不同设备，如何避免冗余的操作，以实现最高效的计算效率，同时避免一些 bug。因此产生了计算图 (Computational Graph)。

​	PyTorch 采用的是动态图机制 (Dynamic Computational Graph)，而 Tensorflow 采用的是静态图机制 (Static Computational Graph)。

​	动态图是==运算和构建同时发生（define by run）==，这种机制由于能够实时得到中间结果的值，使得调试更容易。也就是可以先计算前面的节点的值，再根据这些值搭建后面的计算图。优点是灵活，易调节，易调试。

<img src="E:\pytorch\Notebook\image\day_01\动态图机制.webp" alt="动态图机制" style="zoom: 67%;" />

<center>动态图机制

​	静态图是先搭建图，然后再输入数据进行运算，==计算图的构建和实际计算分开==。优点是高效，因为静态计算是通过先定义后运行的方式，之后再次运行的时候就不再需要重新构建计算图，所以速度会比动态图更快。但是不灵活。TensorFlow 每次运行的时候图都是一样的，是不能够改变的，所以不能直接使用 Python 的 while 循环语句，需要使用辅助函数 tf.while_loop 写成 TensorFlow 内部的形式。

## 3、Pytorch优点

- ==简洁==：PyTorch的设计==追求最少的封装==，尽量避免重复造轮子。不像 TensorFlow 中充斥着session、graph、operation、name_scope、variable、tensor、layer等全新的概念，PyTorch 的设计遵循tensor→variable(autograd)→nn.Module 三个由低到高的抽象层次，分别代表高维数组（张量）、自动求导（变量）和神经网络（层/模块），而且这三个抽象之间联系紧密，可以同时进行修改和操作。==简洁的设计带来的另外一个好处就是代码易于理解==。PyTorch的源码只有TensorFlow的十分之一左右，更少的抽象、更直观的设计使得PyTorch的源码十分易于阅读。
- ==速度==：PyTorch 的灵活性不以速度为代价，在许多评测中，PyTorch 的速度表现胜过 TensorFlow和Keras 等框架。框架的运行速度和程序员的编码水平有极大关系，但同样的算法，使用PyTorch实现的那个更有可能快过用其他框架实现的。
- ==易用==：PyTorch 是所有的框架中面向对象设计的最优雅的一个。PyTorch的面向对象的接口设计来源于Torch，而Torch的接口设计以灵活易用而著称，Keras作者最初就是受Torch的启发才开发了Keras。PyTorch继承了Torch的衣钵，尤其是API的设计和模块的接口都与Torch高度一致。PyTorch的设计最符合人们的思维，它让用户尽可能地专注于实现自己的想法，即所思即所得，不需要考虑太多关于框架本身的束缚。
- ==活跃的社区==：PyTorch 提供了完整的文档，循序渐进的指南，作者亲自维护的论坛 供用户交流和求教问题。Facebook 人工智能研究院对 PyTorch 提供了强力支持，作为当今排名前三的深度学习研究机构，FAIR的支持足以确保PyTorch获得持续的开发更新，不至于像许多由个人开发的框架那样昙花一现。

## 4、Pytorch整体架构

​	PyTorch(Caffe2) 通过混合前端，==分布式训练==以及工具和库生态系统实现快速，灵活的实验和高效生产。PyTorch 和 TensorFlow 具有不同计算图实现形式，TensorFlow 采用静态图机制(预定义后再使用)，PyTorch采用动态图机制(运行时动态定义)。PyTorch 具有以下高级特征：

- 混合前端:新的混合前端在急切模式下提供易用性和灵活性，同时无缝转换到图形模式，以便在C ++运行时环境中实现速度，优化和功能。
- 分布式训练:通过利用本地支持集合操作的异步执行和可从Python和C ++访问的对等通信，优化了性能。
- Python优先: PyTorch为了深入集成到Python中而构建的，因此它可以与流行的库和Cython和Numba等软件包一起使用。
- 丰富的工具和库:活跃的研究人员和开发人员社区建立了丰富的工具和库生态系统，用于扩展PyTorch并支持从计算机视觉到强化学习等领域的开发。  
- 本机ONNX支持:以标准ONNX（开放式神经网络交换）格式导出模型，以便直接访问与ONNX兼容的平台，运行时，可视化工具等。   
- C++前端：C++前端是PyTorch的纯C++接口，它遵循已建立的Python前端的设计和体系结构。它旨在实现高性能，低延迟和裸机C++应用程序的研究。 使用GPU和CPU优化的深度学习张量库。

<img src="E:\pytorch\Notebook\image\day_01\Pytoch架构.jpg" style="zoom:80%;" />

<center>Pytorch整体架构图

## 5、Pytorch结构

​	pytroch主要有==torch和torchvision==两大块，但是torchvision也是基于torch开发专门处理计算机视觉或者图像方面的库，所以我们主要还是分析torch这个包。torch这个包就是pytorch库的主体了，它主要实现了对Tensor结构的多种数学操作，比如==切片==之类的。然后这个包还包括了下面很多个包。

![](E:\pytorch\Notebook\image\day_01\PyTroch架构图.png)

<center>Pytorch架构图

- 因为整个torch操作的大部分数据类型都是Tensor，所以torch.Tensor呢可以说是整个torch的结构基础，在这个包里定义了tensor这个数据结构，包括tensor的维度、数据类型等。
- Storage是torch的存储基础，这个包管理着数据要怎么存放在硬件里面，比如是以bite的形式存储还是以char的形式存储，是存储在cpu中还是gpu中等等。
- autograd，负责网络中的求导。
- optim，负责优化网络参数，使得网络能够一步步拟合所需要的函数。
- cuda，使用gpu加速计算的重要库。
- utils，应该是utility的简称，就是效率工具的意思，可以帮助我们更好的训练网络。它下面的Data子包包括了Dataset模块，可以让我们简单的调用数据集，Dataloader则可以让我们方便的使用数据集。



<img src="E:\pytorch\Notebook\image\day_01\nn模块.png" style="zoom: 50%;" />

<center>nn模块

​	最后来看一下nn这个模块，主要用来负责搭建网络，它下面又有很多子模块：

- Parameter负责管理网络中的参数，比如是否需要求导。
- init负责各种参数的初始化。
- functional则包含了很多函数，比如激活函数ReLU、sigmoid，之类的。Nomalizations则是归一化函数，LossFunctions就是均方误差、交叉熵之类的损失函数了。
- Containers是nn的主体，用来存放各种上面或者下面的结构的。如果你要写一个网络模型，用类的方式来写，就得继承这个Module，相当于让你的网络类也具有容器的功能，可以往里放各种结构，比如全连接层，激活函数，卷积层之类的。
- sequential是更轻量化一点的容器，不过效果跟Module差不多，不细讲。
- 然后就是最核心的网络层了，nn里面也集成了很多常用的网络层，比如全连接层Linear，然后一二三维的卷积层、或者一二三维的池化层之类的。
- nn里面也给我们直接集成了现在常用的网络模型，比如RNN、LSTM等，方便我们直接调用。

## 6、使用具体思路

​	根据==Pytorch整体架构图==，我们将pyTorch这么多模块分成了4层，分别是数据存储层、网络搭建层、优化层、应用层。

1. **数据存储层**由Storage模块和Tensor模块组成，定义了torch操作的数据的存储结构和数据结构。
2. **网络搭建层**全靠nn模块撑起来。如果要自己搭建网络的话，就使用Container声明一个容器，然后往里面放我们的网络层还有各种激活函数，归一化函数之类的。也可以直接使用nn里面现成的网络模型。然后可以用Parameter和init去操作这两种网络类型中的参数。
3. **优化层**由Autograd和optim组成，有了网络，我们还需要想想怎么让网络中的参数逼近我们想要的函数，这时候就需要用到优化层，其功能就是优化我们的网络使它向目标网络靠近。
4. **应用层**就是utility包里面的Dataset模块和Dataloader模块，一个负责存取数据集，一个负责调用数据集。当然这一层我们也可以用自己手动输入数据代替，不过如果是大量的数据还是把数据制作成数据集会更加方便操作一点。

## 7、Pytorch与Tensorflow的区别

​	TensorFlow是由Google开发的深度学习框架，最初以静态计算图著称，但后来也引入了动态图机制。它支持多种编程语言，包括Python、C++和Java，并拥有强大的分布式计算能力。其与PyTorch之间的主要区别如下：

1. ==计算图的构建方式==

   ​	PyTorch使用动态计算图，这意味着计算图是根据代码的实际执行过程动态构建的。这种方式使得调试和编写代码更加方便，但也导致了一些性能上的损失；TensorFlow最初采用的是静态计算图，即需要在构建阶段定义完整的计算图，然后才能执行。这种方式可以进行更多的优化，提高性能，但在调试和开发过程中较为繁琐。

2. ==代码的可读性和易用性==

   ​	由于PyTorch使用Python作为主要接口，它的代码具有很高的可读性和易用性。借助Python的简洁语法，开发者可以更快地构建和调试模型。TensorFlow的代码相对较复杂，特别是在较早的版本中。不过，随着TensorFlow 2.0的发布，它引入了Keras API，使得代码编写更加简单和直观。

3. ==动态性和静态性的权衡==

   ​	动态计算图使得PyTorch在调试和开发过程中更加灵活，可以进行动态的控制流操作。这意味着我们可以在运行时改变模型的结构和参数，方便地进行调试和实验。相比之下，TensorFlow的静态计算图可以在构建阶段进行更多的优化，提高了性能和效率。它适用于需要高度优化和部署到生产环境的情况。

4. ==社区和生态系统==

   ​	PyTorch在近年来迅速发展，并拥有庞大而活跃的社区。这意味着有大量的开源项目、教程和资源可供使用，可以更好地支持开发者的需求。TensorFlow作为一个由Google支持的框架，也有强大的社区和生态系统。它的用户群体广泛，有更多的工具和库可供选择。

​	总的来说，PyTorch以其动态计算图和易用性在==研究和实验==中受到青睐，而TensorFlow则以其静态计算图和==更广泛的部署能力==在**工业界**广泛使用。然而，随着两个框架的不断发展，它们之间的界限也在逐渐模糊。我们可以根据具体的需求和场景选择合适的框架，并利用它们提供的丰富工具和资源进行深度学习的研究和开发。

