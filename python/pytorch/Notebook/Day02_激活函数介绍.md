## 1、什么是激活函数

​	==激活函数==（==Activation Function==）是一种添加到人工神经网络中的函数，**旨在**==增强网络模型表达非线性模型的能力==。  在神经元中，输入的input经过一系列加权求和后 传入另一个函数，这个函数就是这里的激活函数，激活函数最终决定了是否传递信号以及要发射给下一个神经元的内容。在人工神经网络中，一个节点的激活函数定义了这个节点在**给定了输入（或输入集合）下的输出。**

## 2、为什么要使用激活函数

​	因为神经网络中每一层的输入输出都是一个线性求和的过程，**下一层的输出只是承接了上一层函数输出的线性变换**。所以，如果没有激活函数，无论构造的神经网络有多复杂，有多少层，最后的输出都是==输入的线性组合==，**纯粹的线性组合并不能够解决更复杂的问题**。而引入激活函数之后，我们发现常见的激活函数都是非线性的，因此也会给神经元引入非线性元素，使得神经网络可以逼近其他的任何非线性函数，这样就可以**使得神经网络能够表达更为复杂的非线性模型**。

## 3、激活函数的性质

- ​	连续并可导（允许少数点上不可导）：可导的激活函数可以利用数值优化的方法来学习网络参数；
- ​	定义域是R：激活函数需要能够映射所有的实数；
- ​	单调增的S型曲线：激活函数只需要增加非线性，并不需要其改变对输入的相应状态；

​	**另外，激活函数及其倒数要尽可能简单，太复杂不利于提高网络计算率，其导函数值域要在一个合适的区间内，不能太大也不能太小，否则会影响训练效率和稳定性**

## 	4、常见的激活函数

​	（1）Sigmoid函数

​	**定义：**sigmod函数也叫作Logistic函数，用于隐层神经单元输出，它可以==将R上的一个实数映射到（0,1）的区间，可以用来做二分类，在特征相差比较复杂或者相差不是特别大的时候效果比较好。其表达式、函数图像和导函数图像如下图所示：

# $\mathrm{f(x)=\frac1{1+e^{-x}}}$	<img src="E:\pytorch\Notebook\image\day_02\Sigmoid函数图像.png" style="zoom: 55%;" /><img src="E:\pytorch\Notebook\image\day_02\Sigmoid导函数图像.png" style="zoom: 53%;" />

​			(a)函数表达式												 	(b)函数图像																(c)导函数图像

​		**优点：**

- Sigmoid函数可微且梯度平滑，避免出现”跳跃“的输出值；
-  适合用于将预测概率作为输出的模型，输出值限定在0-1；
- 对每个神经元的输出进行了归一化；

​		**缺点：**

- ==梯度消失：== Sigmoid 激活函数进行反向传播时，输出接近 0 或 1 的神经元其梯度趋近于 0，这种情况称神经元叫作饱和神经元，此时神经元的权重不会更新，与此类神经元相连的神经元的权重也更新得很慢，该问题叫作梯度消失。果一个大型神经网络包含 Sigmoid 神经元，而其中很多个都处于饱和状态，那么该网络无法执行反向传播。
- ==不以零为中心：==Sigmoid 输出不以零为中心的,，输出恒大于0，非零中心化的输出会使得其后一层的神经元的输入发生偏置偏移（Bias Shift），并进一步使得梯度下降的收敛速度变慢。
- ==计算成本高昂：==exp() 函数与其他非线性激活函数相比，计算成本高昂，计算机运行起来速度较慢。



（2）Tanh函数（双曲正切函数）

​	**定义：**Tanh 激活函数又叫作双曲正切激活函数（hyperbolic tangent activation function）。与 Sigmoid 函数类似，Tanh 函数也使用真值，但 Tanh 函数将其压缩至（-1 ，1）的区间内。与 Sigmoid 不同，Tanh 函数的输出以零为中心。其表达式、函数和导函数如下：

# $f(x)=\tanh(x)=\frac{e^x-e^{-x}}{e^x+e^{-x}}=\frac2{1+e^{-2x}}-1$

<center>函数表达式

​									<img src="E:\pytorch\Notebook\image\day_02\Tanh函数图像.png" style="zoom:50%;" /> <img src="E:\pytorch\Notebook\image\day_02\Tanh导函数图像.png" style="zoom:50%;" />

​														 	(a)函数图像													(b)导函数图像

**优点：**

- Tanh函数的值域是(-1, 1)，是一个0均值函数，所以一般来说其性能高于Sigmoid函数；
- 负数输入被当作负值，零输入值的映射接近零，正数输入被当作正值；

**缺点：**

- 与sigmoid类似，Tanh 函数也会有**梯度消失**的问题，因此在饱和时（x很大或很小时）也会“杀死”梯度。

**注意：**==在一般的二元分类问题中，tanh 函数用于隐藏层，而 sigmoid 函数用于输出层，但这并不是固定的，需要根据特定问题进行调整。==



（3）Relu函数

​	**定义：**ReLU函数又称为修正线性单元（Rectified Linear Unit），是一种分段线性函数，其弥补了sigmoid函数以及tanh函数的梯度消失问题，在目前的深度神经网络中被广泛使用。ReLU函数本质上是一个斜坡（ramp）函数，公式、函数和导函数图像如下：

### $\mathrm{Relu}(y)=\begin{cases}y,&y>0\\0,&y\leq0\end{cases}$ <img src="E:\pytorch\Notebook\image\day_02\Relu函数图像.png" style="zoom:50%;" /> <img src="E:\pytorch\Notebook\image\day_02\Relu导函数图像.png" style="zoom:50%;" />

​		(a)函数表达式										 		 	(b)函数图像														(c)导函数图像

**优点：**

- 计算简单，收敛速度快；
- 非饱和函数，避免梯度消失；
- 动态控制神经元的状态，有选择的激活（大于或小于零，等于零被抑制）神经元状态，把这种性质称为稀疏性，如果输入的参数发生变化，只有少部分神经元需要改变状态，不需要全局调整（==信息耦合程度低==），适合在实际中应用降低信息耦合程度；
- 这种动态开启和关闭神经元的做法可以支持不同输入维度和中间层维度的特征学习（表达维度尺寸可变）；

​	**缺点：**

- 非零均值函数，影响网络收敛效果（可以在输出时增加归一化方法解决）；
- Relu函数没有上界，如果线性单元输出过大，或者网络是循环结构可能会导致梯度累计超出计算机的数值上限（==梯度爆炸现象==），需要在参数初始化或重新搭建网络结构来解决；



Relu函数的一些改进：Leaky ReLU、 Parametric ReLU



在正负无穷函数梯度为零的函数称为饱和函数

